# 2024 êµ­ë¦½êµ­ì–´ì› ì¸ê³µì§€ëŠ¥ì˜ í•œêµ­ì–´ ëŠ¥ë ¥ í‰ê°€

ëŒ€í™” ë§¥ë½ ì¶”ë¡  (ê°€ ìœ í˜•) - **ëª¨ë‘ì˜ ë§ë¿¡ì¹˜** íŒ€ ğŸŠ
> ë¦¬ë”ë³´ë“œ 2ìœ„ ëª¨ë¸- **'ì •ë§ë¿¡'**


<br>
ë³¸ ë¦¬í¬ì§€í† ë¦¬ëŠ” 'ëŒ€í™” ë§¥ë½ ì¶”ë¡ 'ì— ëŒ€í•œ ëª¨ë‘ì˜ ë§ë¿¡ì¹˜ íŒ€ì˜ ì œì¶œ ëª¨ë¸ì˜ í•™ìŠµê³¼ í‰ê°€ë¥¼ ì¬í˜„í•˜ê¸° ìœ„í•œ ì½”ë“œë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

<br>
<p align="center">
<img src="./asset/guklip.png" width="300">
</p>

<br>

_í•™ìŠµ ë° ì¶”ë¡ ì˜ ì‹¤í–‰ ë°©ë²•ì€ ì•„ë˜ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤._
</br>

## Table of Contents  
1. [ì†Œê°œ](#1._ì†Œê°œ)  
2. [ë°ì´í„°ì…‹ ì†Œê°œ](#2._ë°ì´í„°ì…‹_ì†Œê°œ) 
3. [EDA](#3._EDA)
4. [ëª¨ë¸ ê°œìš”](#4._ëª¨ë¸_ê°œìš”)
<br> a. [ì‚¬ìš© ëª¨ë¸ ì„ íƒ](##a._ì‚¬ìš©_ëª¨ë¸_ì„ íƒ)
<br> b. [CoT](##b._CoT)
<br> c. [Persona](##c.Persona)
5. [ì‚¬ìš© í”„ë¡¬í”„íŠ¸](#5._ì‚¬ìš©_í”„ë¡¬í”„íŠ¸)
... ì¶”ê°€ ì˜ˆì •

---
<br>

## 1. ì†Œê°œ
â€˜ëŒ€í™” ë§¥ë½ ì¶”ë¡ ' ê³¼ì œì—ì„œ **â€˜ê°€'** ìœ í˜•ìœ¼ë¡œ ì™¸ë¶€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ë°ì´í„° ì¦ê°• ì‚¬ìš©ì´ ë¶ˆê°€í•©ë‹ˆë‹¤.
<br>ë³¸ ê³¼ì œëŠ” ì£¼ì–´ì§„ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, íŠ¹ì •ëœ ëŒ€ìƒ ë°œí™”ë¡œë¶€í„° ë‹¤ì„¯ ê°€ì§€ì˜ ì¶”ë¡ ë¬¸ ìœ í˜•ì¸ â€˜ì›ì¸â€™, â€˜í›„í–‰ ì‚¬ê±´â€™, â€˜ì „ì œ ì¡°ê±´â€™, â€˜ë‚´ì  ë™ê¸°â€™, â€˜ê°ì • ë°˜ì‘â€™ ì¤‘ í•˜ë‚˜ì— ìƒì‘í•˜ëŠ” ë¬¸ì¥ì„ ì •í™•í•˜ê²Œ ì¶”ë¡ í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
ëª¨ë¸ì€ ì„¸ ê°œì˜ ì¶”ë¡  ì˜µì…˜ ì¤‘ ê°€ì¥ ì í•©í•œ ë‹µì„ ì„ íƒí•´ì•¼ í•˜ë©°, ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ëŒ€í™” ë§¥ë½ ì´í•´ ëŠ¥ë ¥ê³¼ ì í•©í•œ ì¶”ë¡  ì„ íƒ ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. </br>

<br>ì €í¬ ëª¨ë¸ì€ ëŒ€íšŒì˜ ê¸°ì¤€ ëª¨ë¸ì¸ **ë¶ˆë¡œì„¬(Bllossom)**-[`teddysum/Korean_CCI_2024`](https://github.com/MLP-Lab/Bllossom) ì„ ê¸°ë°˜í•˜ì—¬ êµ¬í˜„ë˜ì—ˆìœ¼ë©°, Bllossomì€ í•œêµ­ì–´ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹ëœ LLaMA3 ê¸°ë°˜ì˜ í•œêµ­ì–´ ëŒ€í™” ì¶”ë¡ ì„ ìœ„í•´ ì„¤ê³„ëœ ëª¨ë¸ì…ë‹ˆë‹¤. 

### ëª¨ë‘ì˜ ë§ë¿¡ì¹˜ íŒ€ ì†Œê°œ
> ì—°ì„¸ëŒ€í•™êµ ë¹…ë°ì´í„° í•™íšŒ 'YBIGTA'ì˜ Data Science íŒ€ ì†Œì† í•™ìƒë“¤

<p align="center">
<img src="./asset/ybigta.png" width="280">
</p>


## 2. ë°ì´í„°ì…‹ ì†Œê°œ
êµ­ë¦½êµ­ì–´ì›ì—ì„œ ì œê³µë˜ëŠ” ëŒ€í™” ë§¥ë½ ì¶”ë¡  ë§ë­‰ì¹˜ ë°ì´í„°ì…‹ì€ ëŒ€í™”ë¬¸, ëŒ€ìƒ ë°œí™”, ì¶”ë¡ ë¬¸ì˜ ìœ í˜•, ì¶”ë¡ ë¬¸ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 
> ë°ì´í„° í˜•ì‹ì˜ ì˜ˆì‹œ
<p align="center">
<img src="./asset/dataset.png" width="450">
</p>


## 3. EDA
ëŒ€í™” ë§¥ë½ ì¶”ë¡  ë§ë­‰ì¹˜ ë°ì´í„°ì…‹ì˜ trainê³¼ testì— ëŒ€í•œ EDA (Exploratory Data Analysis) ê²°ê³¼ëŠ” [`eda/train`](./eda/train) ì™€ [`eda/test`](./eda/test) ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

> ëŒ€í™” ë§¥ë½ ì¶”ë¡  ë§ë­‰ì¹˜ train ë°ì´í„°ì˜ EDA ê²°ê³¼ ì˜ˆì‹œ
> <img src="./eda/train/train_utterance_wordcloud.png" width="550">

> íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ì„ í†µí•œ ì¸ì‚¬ì´íŠ¸ë¥¼ í†µí•œ ì „ì²˜ë¦¬ ì§„í–‰


## 4. ëª¨ë¸ ê°œìš”

### a. ì‚¬ìš© ëª¨ë¸ ì„ íƒ

ëŒ€í™” ë§¥ë½ ì¶”ë¡  ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœ ëª¨ë¸ì˜ ì¢…ë¥˜ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

- [`MLP-KTLim/llama-3-Korean-Bllossom-8B`](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B)
- [`x2bee/POLAR-14B-v0.2`](https://huggingface.co/x2bee/POLAR-14B-v0.2)
- [`rtzr/ko-gemma-2-9b-it`](https://huggingface.co/rtzr/ko-gemma-2-9b-it)
- [`beomi/Solar-Ko-Recovery-11B`](https://huggingface.co/beomi/Solar-Ko-Recovery-11B)
- [`yanolja/EEVE-Korean-Instruct-10.8B-v1.0`](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)
- [`Qwen/Qwen2-7B`](https://huggingface.co/Qwen/Qwen2-7B)
- [`Qwen/Qwen2-7B-Instruct`](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
- [`spow12/Qwen2-7B-ko-Instruct-orpo-ver_2.0_wo_chat`](https://huggingface.co/spow12/Qwen2-7B-ko-Instruct-orpo-ver_2.0_wo_chat)

<br>
ìœ„ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ì„ ì • ê¸°ì¤€ê³¼ ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤.
</br>

1. í•œêµ­ì–´ ì¶”ë¡  íƒœìŠ¤í¬ë¥¼ ìœ„í•´ [í•œêµ­ì–´ ëª¨ë¸ì˜ ì˜¤í”ˆ ë¦¬ë”ë³´ë“œ](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)ë¥¼ ì°¸ê³ í•˜ì—¬ ëª¨ë¸ì„ ì„ ì •í•˜ì˜€ìŠµë‹ˆë‹¤.
2. ë¦¬ë”ë³´ë“œì˜ ì„±ëŠ¥ê³¼ ì‹¤ì œ ì‚¬ìš© í™˜ê²½ì—ì„œ ì„±ëŠ¥ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ ë°œíœ˜ë¥¼ ìœ„í•˜ì—¬ í—ˆê¹…í˜ì´ìŠ¤ ë‹¤ìš´ë¡œë“œ ìˆ˜, í•œêµ­ì–´ NLP ì˜¤í”ˆí†¡ë°©ì—ì„œ ë‹¤ìˆ˜ ì–¸ê¸‰ëœ ëª¨ë¸ì„ ê³ ë ¤í•˜ì—¬ ìµœì¢… 8ê°œ ëª¨ë¸ì„ ì„ ì •í•˜ì˜€ìŠµë‹ˆë‹¤.
3. ìµœì¢…ì ìœ¼ë¡œ í•œêµ­ì–´ ì¶”ë¡ ì„ ì•ˆì •ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ìœ¼ë¡œ ì œê³µí•˜ê³ ì ì—¬ëŸ¬ ëª¨ë¸ì„ ì„ ì • í›„ ëª¨ë¸ì˜ ì¥ì ë§Œì„ ì„ ë³„í•˜ì—¬ ì•™ìƒë¸”í•˜ëŠ” ë°©ë²•ì„ ì±„íƒí•˜ì˜€ìŠµë‹ˆë‹¤.
4. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ë¯¸ì„¸ ì¡°ì •ì„ ë” ê²½ì œì ì´ê³  ì‹¤ìš©ì ìœ¼ë¡œ ë§Œë“¤ì–´, í° ëª¨ë¸ë„ ì‘ì€ ìì›ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ LoRAì™€ QLoRAë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

### b. í•˜ì´í¼ íŒŒë¼ë¯¸í„°


| Backbone Model | PEFT Method | Learning Rate | Batch Size | Epoch |
|:--------------|-------------------:|-----:|------------:|:-----:|
| [`MLP-KTLim/llama-3-Korean-Bllossom-8B`](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B) | - | 2e-5 | 1 | 15 |
| [`x2bee/POLAR-14B-v0.2`](https://huggingface.co/x2bee/POLAR-14B-v0.2) | QLoRA | 2e-5 | 1 | 18 |
| [`rtzr/ko-gemma-2-9b-it`](https://huggingface.co/rtzr/ko-gemma-2-9b-it) | QLoRA | 2e-5 | 2 | 23 |
| [`beomi/Solar-Ko-Recovery-11B`](https://huggingface.co/beomi/Solar-Ko-Recovery-11B) | QLoRA | 2e-5 | 1 | 11 |
| [`yanolja/EEVE-Korean-Instruct-10.8B-v1.0`](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0) | QLoRA | 2e-5 | 1 | 14 |
| [`Qwen/Qwen2-7B`](https://huggingface.co/Qwen/Qwen2-7B) | - | 2e-5 | 1 | 6 |
| [`Qwen/Qwen2-7B-Instruct`](https://huggingface.co/Qwen/Qwen2-7B-Instruct) | - | 2e-5 | 1 | 8 |
| [`spow12/Qwen2-7B-ko-Instruct-orpo-ver_2.0_wo_chat`](https://huggingface.co/spow12/Qwen2-7B-ko-Instruct-orpo-ver_2.0_wo_chat) | QLoRA | 2e-5 | 1 | 5 |


### c. Parameter Efficient Fine Tuning (PEFT)
ëª¨ë¸ í›ˆë ¨ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´, íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ìœ¼ë¡œ fine-tuning í•˜ëŠ” Parameter-Efficient-Fine-Tuning(PEFT)ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì— í•„ìš”í•œ ë©”ëª¨ë¦¬ ìš©ëŸ‰ê³¼ ê³„ì‚°ëŸ‰ì˜ í¬ê¸°ë¥¼ ì¤„ì˜€ìŠµë‹ˆë‹¤. PEFTì—ì„œë„ [LoRA](https://arxiv.org/pdf/2106.09685)ì™€ [QLoRA](https://arxiv.org/pdf/2305.14314)ë¼ëŠ” ë‘ ê°€ì§€ ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ìì› ì†Œëª¨ë¥¼ ìµœì†Œí™”í•˜ì˜€ìŠµë‹ˆë‹¤.

- **LoRA (Low-Rank Adaptation)**: Low-rank factorization ë°©ë²•ì„ í™œìš©í•˜ì—¬ LLMì˜ linear layerì— ëŒ€í•œ ì—…ë°ì´íŠ¸ë¥¼ ê·¼ì‚¬í™”í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•˜ì§€ ì•Šê³ ë„ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
- **QLoRA (Quantized LoRA Adapters)**: LoRAì˜ ê°œë…ì„ ì–‘ìí™”í•˜ì—¬ ë©”ëª¨ë¦¬ì™€ ì—°ì‚° ìì›ì„ ì ˆì•½í•˜ë©´ì„œë„ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ ì ì‘ì‹œí‚µë‹ˆë‹¤. ì´ëŠ” ë¦¬ì†ŒìŠ¤ê°€ ì œí•œëœ í™˜ê²½ì—ì„œë„ íš¨ìœ¨ì ì¸ ëª¨ë¸ ì¡°ì •ì´ ê°€ëŠ¥í•˜ê²Œ í•´ì£¼ì—ˆìŠµë‹ˆë‹¤.


<p align="center">
<img src="./asset/qlora.png" width="400">
</p>



## 5. ì‚¬ìš© í”„ë¡¬í”„íŠ¸
<p align="center">
<img src="./asset/prompt.png" width="500">
</p>

### a. Textgrad
- TEXTGRADëŠ” ë”¥ëŸ¬ë‹ ì‘ì—…ì˜ ê²°ê³¼ë¬¼ì„ LLMì´ í‰ê°€í•˜ì—¬ ì—­ì „íŒŒ(back propagation)ë¡œ ìˆ˜ì •í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¡œ, Solution, Code Snippetë¿ë§Œ ì•„ë‹ˆë¼ Prompt Optimizationë„ ì§€ì›í•©ë‹ˆë‹¤.

<br>
<p align="center">
<img src="./asset/textgrad.png" width="230">
</p>
</br>

- í•´ë‹¹ í”„ë ˆì„ì›Œí¬ëŠ” ì£¼ì–´ì§„ ì‘ì—…ì— ëŒ€í•´ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ëª¨ë“ˆì— ì§ì ‘ ìš°ë¦¬ì˜ ì‘ì—…ì„ ì‚½ì…í•˜ì—¬ ìˆ˜ì •í•´ì•¼ í•˜ì§€ë§Œ ëª¨ë¸ê³¼ íƒœìŠ¤í¬ì— í•´ë‹¹í•˜ëŠ” ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ LLMì´ ì°¾ì•„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### b. CoT
- [Chain-of-thought](https://arxiv.org/pdf/2201.11903) (Wei et al., 2022) í”„ë¡¬í”„íŠ¸ëŠ” ì¤‘ê°„ ì¶”ë¡  ë‹¨ê³„ë¥¼ í†µí•´ ë³µì¡í•œ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ê²Œ í•¨. ì´ëŠ” ë‹¨ê³„ë¥¼ í†µí•´ í•œêµ­ì–´ ë§¥ë½ê³¼ í˜•íƒœë¥¼ ê³ ë ¤í•˜ì—¬ ì¶”ë¡  ì„±ëŠ¥ í–¥ìƒì´ ê°€ëŠ¥í•˜ê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤.
<p align="center">
<img src="./asset/cot.png" width="500">
</p>

### c. Persona
- [Persona](https://arxiv.org/pdf/2302.11382) (White et al., 2022) í”„ë¡¬í”„íŠ¸ ë°©ë²•ì€ LLMì—ê²Œ íŠ¹ì •í•œ ì—­í• ì„ ë¶€ì—¬í•˜ì—¬ ì—­í• ì— ê±¸ë§ëŠ” ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤.


### d. Categorized
- ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„°ë¥¼ í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ì„¤ëª…ê³¼ ë§ë¶™ì—¬ í•™ìŠµí•˜ê²Œ í•¨ìœ¼ë¡œì¨ ëŒ€í™”ì˜ ì£¼ìš” ë§¥ë½ê³¼ íë¦„ì„ ê³ ë ¤í•˜ê²Œ í•¨. ì´ëŠ” ê°ì íŠ¹ì„±ì„ ê°€ì§„ ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ ê³ ë ¤í•˜ì—¬ í•™ìŠµ ë° ì¶”ë¡ í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.

<p align="center">
<img src="./asset/prompt_per_cat.png" width="460">
</p>

### e. Korean & English
- ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•˜ì—¬ ì˜ì–´ë¡œë§Œ í”„ë¡¬í”„íŒ…í•˜ê±°ë‚˜, í•œêµ­ì–´ë¡œë§Œ í”„ë¡¬í”„íŒ…í•˜ê±°ë‚˜, ì˜ì–´ì™€ í•œêµ­ì–´ë¥¼ í•©ì³ì„œ ì‚¬ìš©í•˜ëŠ” ë“± ì—¬ëŸ¬ ë°©ì‹ì„ í˜¼ìš©í•˜ì—¬ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.


## 6. ORPO
ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´, '[ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/pdf/2403.07691)' ë…¼ë¬¸ì—ì„œ ì†Œê°œëœ ORPO ë°©ì‹ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. 
<br>ORPO ë°©ì‹ì€ "Odds Ratio Preference Optimization"ì˜ ì•½ìë¡œ, reference model ì—†ì´ë„ ì„ í˜¸ë„ë¥¼ ìµœì í™”í•˜ëŠ” ì ‘ê·¼ë²•ìœ¼ë¡œ, ì¶”ë¡ ë¬¸ ìœ í˜•ë³„ë¡œ ëª¨ë¸ì˜ ì„ íƒì„ ë”ìš± ì •êµí•˜ê²Œ ì¡°ì •í•˜ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤. </br>

ê¸°ì¡´ RL ë°©ì‹ê³¼ì˜ ë¹„êµ:

![ORPO ë¹„êµ ì‚¬ì§„](./asset/orpo.png)

Forward pass ê°€ ê¸°ì¡´ì˜ DPOì™€ RLHF ë°©ì‹ì— ë¹„í•´ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„ìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ì™€ ì—°ì‚° íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

ì €í¬ëŠ” ORPO ëŠ” `Qwen2` ëª¨ë¸ì— ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

## 7. Ensemble
í•œêµ­ì–´ ì¶”ë¡ ì„ ì•ˆì •ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ìœ¼ë¡œ ì œê³µí•˜ê³ ì ì—¬ëŸ¬ ëª¨ë¸ì„ ì„ ì • í›„ ëª¨ë¸ì˜ ì¥ì ë§Œì„ ì„ ë³„í•˜ì—¬ ì•™ìƒë¸”í•˜ëŠ” Hard Voting ë°©ë²•ì„ ì±„íƒí•˜ì˜€ìŠµë‹ˆë‹¤.


<p align="center">
<img src="./asset/ensemble.png">
</p>



## 8.ë ˆí¬ì§€í† ë¦¬ êµ¬ì¡° (Repository Structure)

```
# í•™ìŠµì— í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ë“¤ì„ ë³´ê´€í•˜ëŠ” ë””ë ‰í† ë¦¬
resource
â””â”€â”€ data

# ì‹¤í–‰ ê°€ëŠ¥í•œ python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë³´ê´€í•˜ëŠ” ë””ë ‰í† ë¦¬
run
â”œâ”€â”€ test.py
â”œâ”€â”€ train.py
â””â”€â”€ train_base.py

# í•™ìŠµì— ì‚¬ìš©ë  í•¨ìˆ˜ë“¤ì„ ë³´ê´€í•˜ëŠ” ë””ë ‰í† ë¦¬
src
â””â”€â”€ data.py

# EDAë¥¼ ìœ„í•œ Jupyter Notebook íŒŒì¼ê³¼ ê²°ê³¼ png íŒŒì¼ë“¤ì„ ë³´ê´€í•˜ëŠ” ë””ë ‰í† ë¦¬
eda
â”œâ”€â”€ train
â”‚   â”œâ”€â”€ train_EDA.ipynb
â”‚   â”œâ”€â”€ .png
â”‚   â””â”€â”€ ...
â””â”€â”€ test
    â”œâ”€â”€ test_EDA.ipynb
    â”œâ”€â”€ .png
    â””â”€â”€ ...

# orpo ì ìš© ëª¨ë¸ì˜ trainì„ ìœ„í•œ íŒŒì¼ë“¤ì„ ë³´ê´€í•˜ëŠ” ë””ë ‰í† ë¦¬
orpo
â”œâ”€â”€ run_orpo.bash
â”œâ”€â”€ orpo_chatx_data.py
â”œâ”€â”€ orpo_data.py
â”œâ”€â”€ orpo_train.py
â””â”€â”€ merged_data_chatx.py

```


## 8. ì‹¤í–‰ ë°©ë²• (How to Run)

#### Miniconda ì„¤ì¹˜
ë¨¼ì € ìƒˆë¡œìš´ conda í™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤.
```bash
$ mkdir -p ~/miniconda3    # ì‚¬ìš©ì í™ˆ ë””ë ‰í† ë¦¬ì— 'miniconda3' ë””ë ‰í† ë¦¬ë¥¼ ìƒì„± (ë””ë ‰í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•´ë„ ì˜¤ë¥˜ ì—†ì´ ì§„í–‰)
$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh    # Miniconda ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œ
$ bash Miniconda3-latest-Linux-x86_64.sh    # ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ Minicondaë¥¼ ì„¤ì¹˜
$ rm ~/miniconda3/miniconda.sh   # ì„¤ì¹˜ í›„ ë¶ˆí•„ìš”í•œ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì„ ì‚­ì œ
```
ì„¤ì¹˜ê°€ ì™„ë£Œëœ í›„, ìƒˆë¡œ ì„¤ì¹˜í•œ Minicondaë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

```bash
$ ~/miniconda3/bin/conda init bash    # bash ì…¸ì— conda ì´ˆê¸°í™” ì„¤ì •ì„ ì¶”ê°€
$ ~/miniconda3/bin/conda init zsh    # zsh ì…¸ì— conda ì´ˆê¸°í™” ì„¤ì •ì„ ì¶”ê°€
$ conda --version    # ì„¤ì¹˜ëœ condaì˜ ë²„ì „ì„ í™•ì¸í•˜ì—¬ ì„¤ì¹˜ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸

``` 
ìƒˆë¡œìš´ í„°ë¯¸ë„ì—ì„œ conda í™˜ê²½ ìƒì„± ë° ì‹¤í–‰

```bash
$ conda create -n Bbung python=3.9   #ìƒˆë¡œìš´ conda í™˜ê²½ì„ ìƒì„±
$ conda activate Bbung    #'Bbung' í™˜ê²½ í™œì„±í™”
``` 

#### í™˜ê²½ ì„¤ì •
```bash
$ git clone https://github.com/seodaegal/Ko_Conversational_Context_Inference.git    # í”„ë¡œì íŠ¸ ë ˆí¬ì§€í† ë¦¬ë¥¼ í´ë¡ 
$ cd Ko_Conversational_Context_Inference    # í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
$ pip install -r requirements.txt    # í”„ë¡œì íŠ¸ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜

```
### ë°ì´í„°ì…‹ ì—…ë¡œë“œ
[ëŒ€íšŒë§¥ë½ì¶”ë¡  ê°€ ìœ í˜•](https://kli.korean.go.kr/benchmark/taskOrdtm/taskDownload.do?taskOrdtmId=144&clCd=END_TASK&subMenuId=sub02) ë‚´ `ëŒ€í™”ë§¥ë½ì¶”ë¡ _ë°ì´í„°.zip`ì„ ë‹¤ìš´ë°›ì•„ì£¼ì„¸ìš”.

ë‹¤ìš´ë°›ì€ íŒŒì¼ì„ [`resource/data`](/resource/data) í´ë”ì— ë„£ìŠµë‹ˆë‹¤.
```bash
$ unzip resource/data/ëŒ€í™”ë§¥ë½ì¶”ë¡ _ë°ì´í„°.zip
```
ì´í›„ 'ëŒ€í™”ë§¥ë½ì¶”ë¡ 'ì„ 'data'ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.
```bash
$ mv ëŒ€í™”ë§¥ë½ì¶”ë¡ _dev.json data_dev.json
$ mv ëŒ€í™”ë§¥ë½ì¶”ë¡ _test.json data_test.json
$ mv ëŒ€í™”ë§¥ë½ì¶”ë¡ _train.json data_train.json
```

ìµœì¢… `resource/data` íŒŒì¼ í˜•ì‹:

```bash
KR-Conversation-Inference
â”œâ”€â”€ resource
â”‚   â”‚ 
â”‚   â”œâ”€â”€ data # ë°ì´í„° íŒŒì¼ í˜•ì‹
â”‚   â”‚   â””â”€â”€ data_dev.json
â”‚   â”‚   â””â”€â”€ data_test.json
â”‚   â”‚   â””â”€â”€ data_train.json
â”‚   â”‚   â””â”€â”€ sample.json
â”‚   â”‚
â”‚   â””â”€â”€ ...  
â””â”€â”€ ... 
```


---
## 9. í‰ê°€ ê²°ê³¼
<p align="center">
<img src="./asset/leaderboard.png">
</p>


## 10. License
| Model | License |
|:--------------|-------------------:|
|Qwen/Qwen2-7B|Apache License| 
|yanolja/EEVE-Korean-10.8B-v1.0|Apache License|
|spow12/Ko-Qwen2-7B-Instruct|CC-BY-NC-4.0|  
|beomi/Solar-Ko-Recovery-11B|Apache License|  
|x2bee/POLAR-14B-v0.2|Apache License| 
|MLP-KTLim/llama-3-Korean-Bllossom-8B|META LLAMA 3 COMMUNITY LICENSE AGREEMENT| 
|rtzr/ko-gemma-2-9b-it|Gemma Terms of Use| 
|spow12/Qwen2-7B-ko-Instruct-orpo-ver_2.0_wo_chat|Creative Commons Attribution Non Commercial 4.0|

## 11. Reference
- [Github teddysum/Korean_CCI_2024](https://github.com/teddysum/Korean_CCI_2024)
- Brown, T. B. (2020). Language models are few-shot learners. NeurIPS 2020.
- Kim, B., Kim, H., Lee, S. W., Lee, G., Kwak, D., Jeon, D. H., ... & Sung, N. (2021). What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers. EMNLP 2021.
- Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.  
- Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 35, 24824-24837.
- Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., ... & Zhou, D. (2022). Self-consistency improves chain of thought reasoning in language models. ICLR 2023.
- Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., ... & Zhu, T. (2023). Qwen technical report. arXiv preprint arXiv:2309.16609.
- White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., ... & Schmidt, D. C. (2023). A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382.
- Kim, S., Choi, S., & Jeong, M. (2024). Efficient and effective vocabulary expansion towards multilingual large language models. arXiv preprint arXiv:2402.14714.
- Yuksekgonul, M., Bianchi, F., Boen, J., Liu, S., Huang, Z., Guestrin, C., & Zou, J. (2024). TextGrad: Automatic" Differentiation" via Text. arXiv preprint arXiv:2406.07496.
